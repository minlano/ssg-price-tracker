#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
í¬ë¡¤ë§ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
- ê° ê¸°ëŠ¥ë³„ ì‚¬ìš© ì „í›„ ì •í™•í•œ ìˆ˜ì¹˜ ë¹„êµ
- ì‹¤ì œ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ì¸¡ì •
"""

import time
import statistics
from typing import List, Dict
from crawler import search_ssg_products_legacy, search_ssg_products
from cache_manager import cache_manager
from concurrent.futures import ThreadPoolExecutor
import threading

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.results = {}
        self.test_keywords = ["ì•„ì´í°", "ë‚˜ì´í‚¤", "ë¼ë©´", "ë…¸íŠ¸ë¶", "ì‚¼ì„±"]
        
    def measure_execution_time(self, func, *args, **kwargs) -> Dict:
        """í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì •í™•íˆ ì¸¡ì •"""
        times = []
        results = []
        
        # 5íšŒ ë°˜ë³µ ì¸¡ì •í•˜ì—¬ í‰ê· ê°’ ê³„ì‚°
        for i in range(5):
            start_time = time.perf_counter()
            result = func(*args, **kwargs)
            end_time = time.perf_counter()
            
            execution_time = end_time - start_time
            times.append(execution_time)
            results.append(result)
            
            print(f"  í…ŒìŠ¤íŠ¸ {i+1}/5: {execution_time:.3f}ì´ˆ, {len(result)}ê°œ ìƒí’ˆ")
            
            # í…ŒìŠ¤íŠ¸ ê°„ ê°„ê²©
            time.sleep(0.5)
        
        return {
            'times': times,
            'avg_time': statistics.mean(times),
            'min_time': min(times),
            'max_time': max(times),
            'std_dev': statistics.stdev(times) if len(times) > 1 else 0,
            'results': results,
            'avg_products': statistics.mean([len(r) for r in results])
        }
    
    def test_sync_vs_async_crawler(self) -> Dict:
        """1. ë™ê¸° vs ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ ì„±ëŠ¥ ë¹„êµ"""
        print("\n" + "="*80)
        print("ğŸ” í…ŒìŠ¤íŠ¸ 1: ë™ê¸° vs ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ ì„±ëŠ¥ ë¹„êµ")
        print("="*80)
        
        keyword = "ì•„ì´í°"
        limit = 5
        
        # ìºì‹œ ì´ˆê¸°í™”
        cache_manager.clear_cache()
        
        print(f"\nğŸŒ ë™ê¸° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ (í‚¤ì›Œë“œ: {keyword}, ê°œìˆ˜: {limit})")
        print("-" * 50)
        sync_results = self.measure_execution_time(
            search_ssg_products_legacy, keyword, limit=limit
        )
        
        # ìºì‹œ ì´ˆê¸°í™”
        cache_manager.clear_cache()
        
        print(f"\nğŸš€ ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ (í‚¤ì›Œë“œ: {keyword}, ê°œìˆ˜: {limit})")
        print("-" * 50)
        async_results = self.measure_execution_time(
            search_ssg_products, keyword, limit=limit
        )
        
        # ì„±ëŠ¥ ê°œì„  ê³„ì‚°
        improvement = ((sync_results['avg_time'] - async_results['avg_time']) / sync_results['avg_time']) * 100
        speed_ratio = sync_results['avg_time'] / async_results['avg_time']
        
        result = {
            'sync_avg_time': sync_results['avg_time'],
            'async_avg_time': async_results['avg_time'],
            'improvement_percent': improvement,
            'speed_ratio': speed_ratio,
            'sync_std_dev': sync_results['std_dev'],
            'async_std_dev': async_results['std_dev']
        }
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"   ë™ê¸° í¬ë¡¤ëŸ¬ í‰ê· : {sync_results['avg_time']:.3f}ì´ˆ (Â±{sync_results['std_dev']:.3f})")
        print(f"   ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ í‰ê· : {async_results['avg_time']:.3f}ì´ˆ (Â±{async_results['std_dev']:.3f})")
        print(f"   ì„±ëŠ¥ ê°œì„ : {improvement:.1f}%")
        print(f"   ì†ë„ ë¹„ìœ¨: {speed_ratio:.2f}x")
        
        return result
    
    def test_cache_performance(self) -> Dict:
        """2. ìºì‹œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*80)
        print("ğŸ’¾ í…ŒìŠ¤íŠ¸ 2: ìºì‹œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ")
        print("="*80)
        
        keyword = "ë‚˜ì´í‚¤"
        limit = 5
        
        # ìºì‹œ ì´ˆê¸°í™”
        cache_manager.clear_cache()
        
        print(f"\n1ï¸âƒ£ ì²« ë²ˆì§¸ ê²€ìƒ‰ (ìºì‹œ ì—†ìŒ) - í‚¤ì›Œë“œ: {keyword}")
        print("-" * 50)
        first_search = self.measure_execution_time(
            search_ssg_products, keyword, limit=limit
        )
        
        print(f"\n2ï¸âƒ£ ë‘ ë²ˆì§¸ ê²€ìƒ‰ (ìºì‹œ ìˆìŒ) - í‚¤ì›Œë“œ: {keyword}")
        print("-" * 50)
        cached_search = self.measure_execution_time(
            search_ssg_products, keyword, limit=limit
        )
        
        # ìºì‹œ íš¨ìœ¨ì„± ê³„ì‚°
        cache_improvement = ((first_search['avg_time'] - cached_search['avg_time']) / first_search['avg_time']) * 100
        cache_speed_ratio = first_search['avg_time'] / cached_search['avg_time'] if cached_search['avg_time'] > 0 else float('inf')
        
        result = {
            'first_search_time': first_search['avg_time'],
            'cached_search_time': cached_search['avg_time'],
            'cache_improvement_percent': cache_improvement,
            'cache_speed_ratio': cache_speed_ratio,
            'first_std_dev': first_search['std_dev'],
            'cached_std_dev': cached_search['std_dev']
        }
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"   ì²« ê²€ìƒ‰ í‰ê· : {first_search['avg_time']:.3f}ì´ˆ (Â±{first_search['std_dev']:.3f})")
        print(f"   ìºì‹œ ê²€ìƒ‰ í‰ê· : {cached_search['avg_time']:.3f}ì´ˆ (Â±{cached_search['std_dev']:.3f})")
        print(f"   ìºì‹œ ê°œì„ : {cache_improvement:.1f}%")
        print(f"   ìºì‹œ ì†ë„: {cache_speed_ratio:.0f}x" if cache_speed_ratio != float('inf') else "   ìºì‹œ ì†ë„: ë¬´í•œëŒ€")
        
        return result
    
    def test_parallel_processing(self) -> Dict:
        """3. ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*80)
        print("ğŸ”¥ í…ŒìŠ¤íŠ¸ 3: ìˆœì°¨ vs ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ")
        print("="*80)
        
        keywords = ["ì•„ì´í°", "ì‚¼ì„±", "ë‚˜ì´í‚¤"]
        limit = 3
        
        # ìºì‹œ ì´ˆê¸°í™”
        cache_manager.clear_cache()
        
        print(f"\n1ï¸âƒ£ ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ - í‚¤ì›Œë“œ: {keywords}")
        print("-" * 50)
        
        def sequential_search():
            results = []
            for keyword in keywords:
                result = search_ssg_products(keyword, limit=limit)
                results.extend(result)
            return results
        
        sequential_results = self.measure_execution_time(sequential_search)
        
        # ìºì‹œ ì´ˆê¸°í™”
        cache_manager.clear_cache()
        
        print(f"\n2ï¸âƒ£ ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ - í‚¤ì›Œë“œ: {keywords}")
        print("-" * 50)
        
        def parallel_search():
            results = []
            with ThreadPoolExecutor(max_workers=len(keywords)) as executor:
                futures = [executor.submit(search_ssg_products, keyword, limit=limit) for keyword in keywords]
                for future in futures:
                    results.extend(future.result())
            return results
        
        parallel_results = self.measure_execution_time(parallel_search)
        
        # ë³‘ë ¬ ì²˜ë¦¬ íš¨ìœ¨ì„± ê³„ì‚°
        parallel_improvement = ((sequential_results['avg_time'] - parallel_results['avg_time']) / sequential_results['avg_time']) * 100
        parallel_speed_ratio = sequential_results['avg_time'] / parallel_results['avg_time']
        
        result = {
            'sequential_time': sequential_results['avg_time'],
            'parallel_time': parallel_results['avg_time'],
            'parallel_improvement_percent': parallel_improvement,
            'parallel_speed_ratio': parallel_speed_ratio,
            'sequential_std_dev': sequential_results['std_dev'],
            'parallel_std_dev': parallel_results['std_dev']
        }
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"   ìˆœì°¨ ì²˜ë¦¬ í‰ê· : {sequential_results['avg_time']:.3f}ì´ˆ (Â±{sequential_results['std_dev']:.3f})")
        print(f"   ë³‘ë ¬ ì²˜ë¦¬ í‰ê· : {parallel_results['avg_time']:.3f}ì´ˆ (Â±{parallel_results['std_dev']:.3f})")
        print(f"   ë³‘ë ¬ ê°œì„ : {parallel_improvement:.1f}%")
        print(f"   ë³‘ë ¬ ì†ë„: {parallel_speed_ratio:.2f}x")
        
        return result
    
    def test_product_name_quality(self) -> Dict:
        """4. ìƒí’ˆëª… í’ˆì§ˆ ê°œì„  í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*80)
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ 4: ìƒí’ˆëª… í’ˆì§ˆ ê°œì„  ë¹„êµ")
        print("="*80)
        
        keywords = ["ë¼ë©´", "ìš´ë™í™”", "í™”ì¥í’ˆ"]
        
        legacy_quality = []
        improved_quality = []
        
        for keyword in keywords:
            print(f"\nğŸ” í‚¤ì›Œë“œ: {keyword}")
            print("-" * 30)
            
            # ìºì‹œ ì´ˆê¸°í™”
            cache_manager.clear_cache()
            
            # ê¸°ì¡´ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
            legacy_products = search_ssg_products_legacy(keyword, limit=5)
            legacy_generic = sum(1 for p in legacy_products if f"{keyword} ê´€ë ¨ ìƒí’ˆ" in p['name'])
            legacy_quality_rate = ((len(legacy_products) - legacy_generic) / len(legacy_products)) * 100 if legacy_products else 0
            legacy_quality.append(legacy_quality_rate)
            
            print(f"   ê¸°ì¡´ í¬ë¡¤ëŸ¬: {len(legacy_products)}ê°œ ì¤‘ {len(legacy_products) - legacy_generic}ê°œ ì •í™• ({legacy_quality_rate:.1f}%)")
            
            # ìºì‹œ ì´ˆê¸°í™”
            cache_manager.clear_cache()
            
            # ê°œì„ ëœ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
            improved_products = search_ssg_products(keyword, limit=5)
            improved_generic = sum(1 for p in improved_products if f"{keyword} ê´€ë ¨ ìƒí’ˆ" in p['name'])
            improved_quality_rate = ((len(improved_products) - improved_generic) / len(improved_products)) * 100 if improved_products else 0
            improved_quality.append(improved_quality_rate)
            
            print(f"   ê°œì„ ëœ í¬ë¡¤ëŸ¬: {len(improved_products)}ê°œ ì¤‘ {len(improved_products) - improved_generic}ê°œ ì •í™• ({improved_quality_rate:.1f}%)")
        
        avg_legacy_quality = statistics.mean(legacy_quality)
        avg_improved_quality = statistics.mean(improved_quality)
        quality_improvement = avg_improved_quality - avg_legacy_quality
        
        result = {
            'legacy_quality_avg': avg_legacy_quality,
            'improved_quality_avg': avg_improved_quality,
            'quality_improvement': quality_improvement,
            'legacy_quality_details': legacy_quality,
            'improved_quality_details': improved_quality
        }
        
        print(f"\nğŸ“Š ì „ì²´ ê²°ê³¼:")
        print(f"   ê¸°ì¡´ í¬ë¡¤ëŸ¬ í‰ê·  í’ˆì§ˆ: {avg_legacy_quality:.1f}%")
        print(f"   ê°œì„ ëœ í¬ë¡¤ëŸ¬ í‰ê·  í’ˆì§ˆ: {avg_improved_quality:.1f}%")
        print(f"   í’ˆì§ˆ ê°œì„ : +{quality_improvement:.1f}%p")
        
        return result
    
    def test_memory_efficiency(self) -> Dict:
        """5. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*80)
        print("ğŸ§  í…ŒìŠ¤íŠ¸ 5: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„")
        print("="*80)
        
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            
            # ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            print(f"ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {initial_memory:.1f} MB")
            
            # ì—¬ëŸ¬ ê²€ìƒ‰ ìˆ˜í–‰
            keywords = ["ì•„ì´í°", "ì‚¼ì„±", "ë‚˜ì´í‚¤", "ë¼ë©´", "ë…¸íŠ¸ë¶"] * 2  # 10íšŒ ê²€ìƒ‰
            
            memory_usage = [initial_memory]
            
            for i, keyword in enumerate(keywords):
                search_ssg_products(keyword, limit=3)
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_usage.append(current_memory)
                
                if (i + 1) % 2 == 0:
                    print(f"   {i+1}íšŒ ê²€ìƒ‰ í›„: {current_memory:.1f} MB (+{current_memory-initial_memory:.1f} MB)")
            
            final_memory = memory_usage[-1]
            max_memory = max(memory_usage)
            memory_increase = final_memory - initial_memory
            
            result = {
                'initial_memory_mb': initial_memory,
                'final_memory_mb': final_memory,
                'max_memory_mb': max_memory,
                'memory_increase_mb': memory_increase,
                'memory_efficiency': 'Excellent' if memory_increase < 50 else 'Good' if memory_increase < 100 else 'Fair'
            }
            
            print(f"\nğŸ“Š ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê²°ê³¼:")
            print(f"   ìµœì¢… ë©”ëª¨ë¦¬: {final_memory:.1f} MB")
            print(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {max_memory:.1f} MB")
            print(f"   ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.1f} MB")
            print(f"   íš¨ìœ¨ì„± í‰ê°€: {result['memory_efficiency']}")
            
            return result
            
        except ImportError:
            print("âš ï¸ psutil ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤")
            return {'error': 'psutil not available'}
    
    def generate_comprehensive_report(self) -> str:
        """ì¢…í•© ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*80)
        print("ğŸ“‹ ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰")
        print("="*80)
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        sync_async_result = self.test_sync_vs_async_crawler()
        cache_result = self.test_cache_performance()
        parallel_result = self.test_parallel_processing()
        quality_result = self.test_product_name_quality()
        memory_result = self.test_memory_efficiency()
        
        # ë³´ê³ ì„œ ìƒì„±
        report = []
        report.append("\n" + "ğŸ¯ ìµœì¢… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ")
        report.append("=" * 80)
        
        # 1. ë¹„ë™ê¸° ì²˜ë¦¬ íš¨ê³¼
        report.append(f"\nğŸš€ 1. ë¹„ë™ê¸° ì²˜ë¦¬ íš¨ê³¼:")
        report.append(f"   â€¢ ë™ê¸° í¬ë¡¤ëŸ¬: {sync_async_result['sync_avg_time']:.3f}ì´ˆ")
        report.append(f"   â€¢ ë¹„ë™ê¸° í¬ë¡¤ëŸ¬: {sync_async_result['async_avg_time']:.3f}ì´ˆ")
        report.append(f"   â€¢ ì„±ëŠ¥ ê°œì„ : {sync_async_result['improvement_percent']:.1f}%")
        report.append(f"   â€¢ ì†ë„ í–¥ìƒ: {sync_async_result['speed_ratio']:.2f}ë°°")
        
        # 2. ìºì‹œ ì‹œìŠ¤í…œ íš¨ê³¼
        report.append(f"\nğŸ’¾ 2. ìºì‹œ ì‹œìŠ¤í…œ íš¨ê³¼:")
        report.append(f"   â€¢ ì²« ê²€ìƒ‰: {cache_result['first_search_time']:.3f}ì´ˆ")
        report.append(f"   â€¢ ìºì‹œ ê²€ìƒ‰: {cache_result['cached_search_time']:.3f}ì´ˆ")
        report.append(f"   â€¢ ìºì‹œ ê°œì„ : {cache_result['cache_improvement_percent']:.1f}%")
        if cache_result['cache_speed_ratio'] != float('inf'):
            report.append(f"   â€¢ ìºì‹œ ì†ë„: {cache_result['cache_speed_ratio']:.0f}ë°°")
        else:
            report.append(f"   â€¢ ìºì‹œ ì†ë„: ë¬´í•œëŒ€ (ì¦‰ì‹œ ì‘ë‹µ)")
        
        # 3. ë³‘ë ¬ ì²˜ë¦¬ íš¨ê³¼
        report.append(f"\nğŸ”¥ 3. ë³‘ë ¬ ì²˜ë¦¬ íš¨ê³¼:")
        report.append(f"   â€¢ ìˆœì°¨ ì²˜ë¦¬: {parallel_result['sequential_time']:.3f}ì´ˆ")
        report.append(f"   â€¢ ë³‘ë ¬ ì²˜ë¦¬: {parallel_result['parallel_time']:.3f}ì´ˆ")
        report.append(f"   â€¢ ë³‘ë ¬ ê°œì„ : {parallel_result['parallel_improvement_percent']:.1f}%")
        report.append(f"   â€¢ ë³‘ë ¬ ì†ë„: {parallel_result['parallel_speed_ratio']:.2f}ë°°")
        
        # 4. ìƒí’ˆëª… í’ˆì§ˆ ê°œì„ 
        report.append(f"\nğŸ“Š 4. ìƒí’ˆëª… í’ˆì§ˆ ê°œì„ :")
        report.append(f"   â€¢ ê¸°ì¡´ í’ˆì§ˆ: {quality_result['legacy_quality_avg']:.1f}%")
        report.append(f"   â€¢ ê°œì„ ëœ í’ˆì§ˆ: {quality_result['improved_quality_avg']:.1f}%")
        report.append(f"   â€¢ í’ˆì§ˆ í–¥ìƒ: +{quality_result['quality_improvement']:.1f}%p")
        
        # 5. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
        if 'error' not in memory_result:
            report.append(f"\nğŸ§  5. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:")
            report.append(f"   â€¢ ë©”ëª¨ë¦¬ ì¦ê°€: {memory_result['memory_increase_mb']:.1f} MB")
            report.append(f"   â€¢ íš¨ìœ¨ì„± í‰ê°€: {memory_result['memory_efficiency']}")
        
        # ì „ì²´ ê²°ë¡ 
        report.append(f"\nğŸ† ì „ì²´ ì„±ëŠ¥ ê°œì„  ìš”ì•½:")
        report.append(f"   â€¢ ğŸ¥‡ ìºì‹œ ì‹œìŠ¤í…œ: {cache_result['cache_improvement_percent']:.0f}% ê°œì„  (ê°€ì¥ íš¨ê³¼ì )")
        report.append(f"   â€¢ ğŸ¥ˆ ë³‘ë ¬ ì²˜ë¦¬: {parallel_result['parallel_improvement_percent']:.0f}% ê°œì„ ")
        report.append(f"   â€¢ ğŸ¥‰ ìƒí’ˆëª… í’ˆì§ˆ: +{quality_result['quality_improvement']:.0f}%p í–¥ìƒ")
        report.append(f"   â€¢ ğŸ… ë¹„ë™ê¸° ì²˜ë¦¬: {sync_async_result['improvement_percent']:.0f}% ê°œì„ ")
        
        # ì‹¤ì œ ì‚¬ìš©ì ê²½í—˜ ê°œì„ 
        total_improvement = (
            sync_async_result['improvement_percent'] +
            cache_result['cache_improvement_percent'] +
            parallel_result['parallel_improvement_percent']
        ) / 3
        
        report.append(f"\nğŸ’¡ ì‹¤ì œ ì‚¬ìš©ì ê²½í—˜:")
        report.append(f"   â€¢ í‰ê·  ì„±ëŠ¥ ê°œì„ : {total_improvement:.0f}%")
        report.append(f"   â€¢ ì²« ê²€ìƒ‰: ~{sync_async_result['async_avg_time']:.1f}ì´ˆ")
        report.append(f"   â€¢ ìºì‹œëœ ê²€ìƒ‰: ~{cache_result['cached_search_time']:.3f}ì´ˆ (ê±°ì˜ ì¦‰ì‹œ)")
        report.append(f"   â€¢ ë‹¤ì¤‘ ê²€ìƒ‰: ë³‘ë ¬ ì²˜ë¦¬ë¡œ {parallel_result['parallel_speed_ratio']:.1f}ë°° ë¹ ë¦„")
        report.append(f"   â€¢ ìƒí’ˆëª… ì •í™•ë„: {quality_result['improved_quality_avg']:.0f}%")
        
        return "\n".join(report)

def main():
    """ë©”ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("ğŸ”¬ SSG í¬ë¡¤ëŸ¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print(f"ì‹œì‘ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    benchmark = PerformanceBenchmark()
    report = benchmark.generate_comprehensive_report()
    
    print(report)
    
    print(f"\nâ±ï¸ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ“Š ëª¨ë“  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()