#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import re
import time
from urllib.parse import quote
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor
import threading
from datetime import datetime, timedelta
from bs4 import BeautifulSoup

# ì„ íƒì  import
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False

try:
    from database_models import Product, SessionLocal, get_db
    from sqlalchemy.orm import Session
    DATABASE_AVAILABLE = True
except ImportError:
    DATABASE_AVAILABLE = False

try:
    from cache_manager import cache_manager
    CACHE_AVAILABLE = True
except ImportError:
    CACHE_AVAILABLE = False
    print("âš ï¸ ìºì‹œ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

class AsyncSSGCrawler:
    """ë¹„ë™ê¸° SSG í¬ë¡¤ëŸ¬"""
    
    def __init__(self, max_concurrent=10):
        self.max_concurrent = max_concurrent
        self.session = None
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        if not AIOHTTP_AVAILABLE:
            return self
            
        connector = aiohttp.TCPConnector(limit=50, limit_per_host=20)
        timeout = aiohttp.ClientTimeout(total=10, connect=5)
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """ë¹„ë™ê¸° í˜ì´ì§€ ìš”ì²­"""
        if not AIOHTTP_AVAILABLE or not self.session:
            # ë™ê¸° ë°©ì‹ìœ¼ë¡œ í´ë°±
            return self.fetch_page_sync(url)
            
        async with self.semaphore:
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        print(f"âš ï¸ HTTP {response.status}: {url}")
                        return None
            except Exception as e:
                print(f"âš ï¸ í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}")
                return None
    
    def fetch_page_sync(self, url: str) -> Optional[str]:
        """ë™ê¸° í˜ì´ì§€ ìš”ì²­ (í´ë°±)"""
        try:
            import requests
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.text
            else:
                print(f"âš ï¸ HTTP {response.status_code}: {url}")
                return None
        except Exception as e:
            print(f"âš ï¸ ë™ê¸° í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}")
            return None
    
    async def search_products_async(self, keyword: str, limit: int = 20) -> List[Dict]:
        """ë¹„ë™ê¸° ìƒí’ˆ ê²€ìƒ‰"""
        start_time = time.time()
        
        # 1. ìºì‹œ í™•ì¸ (ìºì‹œ ë§¤ë‹ˆì €ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if CACHE_AVAILABLE:
            cached_results = cache_manager.get_cached_results(keyword, limit)
            if cached_results:
                return cached_results[:limit]
        
        
        # 2. ì‹¤ì œ í¬ë¡¤ë§ ìˆ˜í–‰
        print(f"ğŸ•·ï¸ ì‹¤ì œ í¬ë¡¤ë§ ì‹œì‘: {keyword}")
        
        try:
            encoded_keyword = quote(keyword)
            search_url = f"https://www.ssg.com/search.ssg?target=all&query={encoded_keyword}&page=1"
            
            # ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­
            html_content = await self.fetch_page(search_url)
            if not html_content:
                return []
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ìƒí’ˆ ë§í¬ ì¶”ì¶œ
            product_links = soup.select('a[href*="itemView.ssg"][href*="itemId="]')
            
            # ê´‘ê³  ë§í¬ ì œì™¸
            filtered_links = []
            for link in product_links[:limit * 5]:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                if ('advertBidId' not in href and 
                    'ADAD' not in link_text and
                    'advertExtensTeryDivCd' not in href):
                    filtered_links.append(link)
                    if len(filtered_links) >= limit * 2:
                        break
            
            print(f"ğŸ”— ìœ íš¨í•œ ìƒí’ˆ ë§í¬ {len(filtered_links)}ê°œ ë°œê²¬")
            
            # ë³‘ë ¬ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
            tasks = []
            for link in filtered_links[:limit * 2]:
                task = self.extract_product_info_async(link, keyword)
                tasks.append(task)
            
            # ëª¨ë“  ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§í•˜ê³  ì¤‘ë³µ ì œê±°
            products = []
            seen_urls = set()
            seen_names = set()
            
            for result in results:
                if isinstance(result, dict) and result.get('name'):
                    # URLê³¼ ì´ë¦„ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
                    url = result.get('url', '')
                    name = result.get('name', '').strip()
                    
                    # ì¤‘ë³µ ì²´í¬ (URL ë˜ëŠ” ì´ë¦„ì´ ê°™ìœ¼ë©´ ì œì™¸)
                    if (url not in seen_urls and 
                        name not in seen_names and 
                        len(name) > 5):
                        
                        products.append(result)
                        seen_urls.add(url)
                        seen_names.add(name)
                        
                        if len(products) >= limit:
                            break
            
            elapsed_time = time.time() - start_time
            print(f"âš¡ ë¹„ë™ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ, {len(products)}ê°œ ìƒí’ˆ")
            
                
            # 3. ìºì‹œì— ì €ì¥ (ìºì‹œê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if products and CACHE_AVAILABLE:
                cache_manager.cache_results(keyword, products, limit)
            
            return products
            
        except Exception as e:
            print(f"âŒ ë¹„ë™ê¸° í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    async def extract_product_info_async(self, link, keyword: str) -> Optional[Dict]:
        """ë¹„ë™ê¸° ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            href = link.get('href')
            if href.startswith('/'):
                product_url = f"https://www.ssg.com{href}"
            else:
                product_url = href
            
            # ìƒí’ˆëª… ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì‚¬ìš©)
            name = self.extract_product_name_fast(link, keyword)
            
            # ê°€ê²© ì¶”ì¶œ
            price = 0
            current = link.parent
            for _ in range(3):
                if current:
                    price_text = current.get_text()
                    price = self.extract_price_fast(price_text)
                    if price > 0:
                        break
                    current = current.parent
                else:
                    break
            
            # ë¸Œëœë“œ ì¶”ì¶œ
            brand = self.extract_brand_fast(link, name)
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_url = None
            current = link.parent
            for _ in range(2):
                if current:
                    img = current.find('img')
                    if img:
                        image_url = img.get('src') or img.get('data-src')
                        if image_url:
                            if image_url.startswith('//'):
                                image_url = f"https:{image_url}"
                            elif image_url.startswith('/'):
                                image_url = f"https://www.ssg.com{image_url}"
                            break
                    current = current.parent
                else:
                    break
            
            return {
                'name': name.strip(),
                'price': price,
                'url': product_url,
                'image_url': image_url,
                'brand': brand,
                'source': 'SSG'
            }
            
        except Exception as e:
            return None
    
    def extract_product_name_fast(self, link, keyword: str) -> str:
        """ë¹ ë¥¸ ìƒí’ˆëª… ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        try:
            # 1. ë§í¬ ìì²´ì—ì„œ ìƒí’ˆëª… ì¶”ì¶œ ì‹œë„
            link_text = link.get_text(strip=True)
            if link_text and len(link_text) > 10 and not self.is_generic_text(link_text):
                cleaned = self.clean_product_name(link_text)
                if len(cleaned) > 10:
                    return cleaned
            
            # 2. ë¶€ëª¨ ìš”ì†Œì—ì„œ ìƒí’ˆëª… ê´€ë ¨ í´ë˜ìŠ¤ ì°¾ê¸°
            current = link.parent
            for level in range(5):  # ë ˆë²¨ ì¶•ì†Œ
                if current:
                    # ìƒí’ˆëª… ê´€ë ¨ í´ë˜ìŠ¤ë“¤
                    name_elements = current.select('.cunit_tit, .item_tit, .prod_tit, .tit, .title')
                    for elem in name_elements:
                        text = elem.get_text(strip=True)
                        if text and len(text) > 10 and not self.is_generic_text(text):
                            cleaned = self.clean_product_name(text)
                            if len(cleaned) > 10:
                                return cleaned
                    
                    current = current.parent
                else:
                    break
            
            # 3. ê°œë³„ ìƒí’ˆ í˜ì´ì§€ì—ì„œ ì •í™•í•œ ìƒí’ˆëª… ê°€ì ¸ì˜¤ê¸° (ìºì‹œëœ ê²½ìš°ë§Œ)
            href = link.get('href', '')
            if href and 'itemId=' in href:
                if href.startswith('/'):
                    product_url = f"https://www.ssg.com{href}"
                else:
                    product_url = href
                
                # ê°„ë‹¨í•œ í˜ì´ì§€ ìš”ì²­ìœ¼ë¡œ ì •í™•í•œ ìƒí’ˆëª… ê°€ì ¸ì˜¤ê¸°
                try:
                    import requests
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }
                    response = requests.get(product_url, headers=headers, timeout=3)
                    if response.status_code == 200:
                        from bs4 import BeautifulSoup
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # SSG ìƒí’ˆ í˜ì´ì§€ì˜ ì •í™•í•œ ìƒí’ˆëª… ì„ íƒì
                        title_selectors = [
                            'h2.cdtl_prd_nm',
                            'h1.cdtl_prd_nm',
                            '.prod_tit',
                            'title'
                        ]
                        
                        for selector in title_selectors:
                            title_elem = soup.select_one(selector)
                            if title_elem:
                                title_text = title_elem.get_text(strip=True)
                                if title_text and title_text != "SSG.COM" and len(title_text) > 10:
                                    return self.clean_product_name(title_text)
                except:
                    pass  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            
            # 4. ê¸°ë³¸ê°’ ë°˜í™˜
            return f"{keyword} ê´€ë ¨ ìƒí’ˆ"
            
        except Exception:
            return f"{keyword} ê´€ë ¨ ìƒí’ˆ"
    
    def is_generic_text(self, text):
        """ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸"""
        if not text:
            return True
        
        text_lower = text.lower()
        
        # ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ íŒ¨í„´ë“¤
        generic_patterns = [
            'í•¨ê»˜ ë³´ë©´ ì¢‹ì€',
            'ê´€ë ¨ ìƒí’ˆ',
            'ì¶”ì²œ ìƒí’ˆ',
            'ì¸ê¸° ìƒí’ˆ',
            'ë² ìŠ¤íŠ¸',
            'í• ì¸',
            'ë¬´ë£Œë°°ì†¡',
            'ë‹¹ì¼ë°°ì†¡',
            'ë¦¬ë·°',
            'ë³„ì ',
            'í‰ì ',
            'ë”ë³´ê¸°',
            'ìì„¸íˆ',
            'ìƒí’ˆì •ë³´',
            'ìƒí’ˆìƒì„¸'
        ]
        
        for pattern in generic_patterns:
            if pattern in text_lower:
                return True
        
        # ë„ˆë¬´ ì§§ê±°ë‚˜ ìˆ«ìë§Œ ìˆëŠ” ê²½ìš°
        if len(text) < 5 or text.isdigit():
            return True
        
        return False
    
    def clean_text_line(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ë¼ì¸ ì •ì œ"""
        if not text:
            return ""
        
        text = re.sub(r'íŒë§¤ê°€ê²©.*', '', text)
        text = re.sub(r'(\d{1,3}(?:,\d{3})*ì›).*', '', text)
        text = re.sub(r'(ë¦¬ë·°|ë³„ì |í• ì¸|ë°°ì†¡|ë¬´ë£Œë°°ì†¡|ë‹¹ì¼ë°°ì†¡|íƒë°°).*', '', text)
        text = re.sub(r'(ì¹´ë“œí• ì¸|ì¦‰ì‹œí• ì¸|ì¶”ê°€í• ì¸|ì‚¬ì€í’ˆ|ì¦ì •).*', '', text)
        text = re.sub(r'(100gë‹¹|ê°œë‹¹|mlë‹¹).*', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def calculate_product_name_score(self, text: str, keyword: str) -> int:
        """ìƒí’ˆëª… ì í•©ì„± ì ìˆ˜ ê³„ì‚°"""
        if not text:
            return 0
        
        score = 10
        text_lower = text.lower()
        keyword_lower = keyword.lower()
        
        if keyword_lower in text_lower:
            score += 50
        
        brands = [
            'apple', 'ì‚¼ì„±', 'samsung', 'lg', 'ë‚˜ì´í‚¤', 'nike', 'adidas', 
            'ë†ì‹¬', 'ì˜¤ëšœê¸°', 'ì‚¼ì–‘', 'íŒ”ë„', 'ë¡¯ë°', 'lotte', 'cj'
        ]
        
        for brand in brands:
            if brand in text_lower:
                score += 30
                break
        
        if re.search(r'[A-Z0-9]{3,}', text):
            score += 15
        
        if any(word in text_lower for word in ['ì›', 'ë¦¬ë·°', 'ë³„ì ', 'í• ì¸', 'ë°°ì†¡']):
            score -= 20
        
        if len(text) < 15 or len(text) > 150:
            score -= 10
        
        return max(0, score)
    
    def extract_price_fast(self, text: str) -> int:
        """ë¹ ë¥¸ ê°€ê²© ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        if not text:
            return 0
        
        # ë‹¤ì–‘í•œ ê°€ê²© íŒ¨í„´ ì‹œë„
        price_patterns = [
            r'íŒë§¤ê°€ê²©\s*(\d{1,3}(?:,\d{3})*)',
            r'ì •ìƒê°€ê²©\s*(\d{1,3}(?:,\d{3})*)',
            r'í• ì¸ê°€\s*(\d{1,3}(?:,\d{3})*)',
            r'(\d{1,3}(?:,\d{3})*)\s*ì›',
            r'ê°€ê²©\s*(\d{1,3}(?:,\d{3})*)',
            r'(\d{1,3}(?:,\d{3})*)'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, text)
            if matches:
                # ê°€ì¥ í•©ë¦¬ì ì¸ ê°€ê²© ì„ íƒ
                prices = []
                for match in matches:
                    try:
                        price = int(match.replace(',', ''))
                        if 1000 <= price <= 50000000:  # í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„
                            prices.append(price)
                    except:
                        continue
                
                if prices:
                    # ì—¬ëŸ¬ ê°€ê²©ì´ ìˆìœ¼ë©´ ì¤‘ê°„ê°’ ì„ íƒ (í• ì¸ê°€ì™€ ì •ê°€ê°€ ì„ì—¬ìˆì„ ìˆ˜ ìˆìŒ)
                    prices.sort()
                    return prices[0] if len(prices) == 1 else prices[len(prices)//2]
        
        return 0
    
    def extract_brand_fast(self, link, product_name: str) -> str:
        """ë¹ ë¥¸ ë¸Œëœë“œ ì¶”ì¶œ"""
        try:
            if product_name:
                known_brands = [
                    'APPLE', 'ì‚¼ì„±', 'SAMSUNG', 'LG', 'ë‚˜ì´í‚¤', 'NIKE', 
                    'ë†ì‹¬', 'ì˜¤ëšœê¸°', 'ì‚¼ì–‘', 'ì•„ë””ë‹¤ìŠ¤', 'ADIDAS'
                ]
                
                product_upper = product_name.upper()
                for brand in known_brands:
                    if brand.upper() in product_upper:
                        return brand
                
                first_word = product_name.split()[0] if product_name.split() else ''
                if first_word and 2 < len(first_word) < 15:
                    return first_word
            
            return 'ë¸Œëœë“œ ì •ë³´ ì—†ìŒ'
            
        except Exception:
            return 'ë¸Œëœë“œ ì •ë³´ ì—†ìŒ'
    
    def clean_product_name(self, name: str) -> str:
        """ìƒí’ˆëª… ì •ì œ"""
        if not name:
            return name
        
        name = re.sub(r'(ë¦¬ë·°\s*\d+|ë³„ì |í• ì¸|ë°°ì†¡)', '', name)
        name = re.sub(r'(ì •ìƒê°€ê²©|íŒë§¤ê°€ê²©|ìµœê³ íŒë§¤ê°€)', '', name)
        name = re.sub(r'(ADADë€\?\s*íˆ´íŒ\s*ì—´ê¸°)', '', name)
        name = re.sub(r'(ê²€ìƒ‰\s*í•„í„°|ë¸Œëœë“œ\s*ì „ì²´ë³´ê¸°)', '', name)
        name = re.sub(r'(\d+ë§Œì›\s*ì´í•˜|\d+ë§Œì›~\d+ë§Œì›)', '', name)
        name = re.sub(r'(ì¸ê¸°|BEST|ì¶”ì²œ|ì„ ë¬¼)', '', name)
        name = re.sub(r'(ìƒˆë²½ë°°ì†¡|ë‹¹ì¼ë°°ì†¡|ë¬´ë£Œë°°ì†¡)', '', name)
        name = re.sub(r'\s+', ' ', name).strip()
        
        if len(name) > 60:
            name = name[:60] + "..."
        
        return name
    
# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ í•¨ìˆ˜ë“¤ ì œê±°ë¨ (DB í…Œì´ë¸” ì—†ìŒ)

# ë¹„ë™ê¸° ê²€ìƒ‰ í•¨ìˆ˜
async def search_products_async(keyword: str, limit: int = 20) -> List[Dict]:
    """ë¹„ë™ê¸° ìƒí’ˆ ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜"""
    async with AsyncSSGCrawler(max_concurrent=10) as crawler:
        return await crawler.search_products_async(keyword, limit)

# ë™ê¸° ë˜í¼ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±)
def search_ssg_products_enhanced(keyword: str, page: int = 1, limit: int = 20) -> List[Dict]:
    """í–¥ìƒëœ SSG ìƒí’ˆ ê²€ìƒ‰ (ë¹„ë™ê¸° + ìºì‹± + DB)"""
    try:
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            results = loop.run_until_complete(search_products_async(keyword, limit))
            return results
        finally:
            loop.close()
            
    except Exception as e:
        print(f"âŒ í–¥ìƒëœ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        # ê¸°ì¡´ ë™ê¸° ë°©ì‹ìœ¼ë¡œ í´ë°±
        from crawler import search_ssg_products
        return search_ssg_products(keyword, page, limit)