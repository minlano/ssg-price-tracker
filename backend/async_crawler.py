#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import re
import time
from urllib.parse import quote
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor
import threading
from datetime import datetime, timedelta
from bs4 import BeautifulSoup

# ì„ íƒì  import
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False
    print("âš ï¸ aiohttp ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë™ê¸° ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

try:
    from database_models import Product, SessionLocal, get_db
    from sqlalchemy.orm import Session
    DATABASE_AVAILABLE = True
except ImportError:
    DATABASE_AVAILABLE = False
    print("âš ï¸ SQLAlchemy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ê¸°ëŠ¥ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")

try:
    from cache_manager import cache_manager
    CACHE_AVAILABLE = True
except ImportError:
    CACHE_AVAILABLE = False
    print("âš ï¸ ìºì‹œ ë§¤ë‹ˆì €ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

class AsyncSSGCrawler:
    """ë¹„ë™ê¸° SSG í¬ë¡¤ëŸ¬"""
    
    def __init__(self, max_concurrent=10):
        self.max_concurrent = max_concurrent
        self.session = None
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        if not AIOHTTP_AVAILABLE:
            return self
            
        connector = aiohttp.TCPConnector(limit=50, limit_per_host=20)
        timeout = aiohttp.ClientTimeout(total=10, connect=5)
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """ë¹„ë™ê¸° í˜ì´ì§€ ìš”ì²­"""
        if not AIOHTTP_AVAILABLE or not self.session:
            # ë™ê¸° ë°©ì‹ìœ¼ë¡œ í´ë°±
            return self.fetch_page_sync(url)
            
        async with self.semaphore:
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        print(f"âš ï¸ HTTP {response.status}: {url}")
                        return None
            except Exception as e:
                print(f"âš ï¸ í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}")
                return None
    
    def fetch_page_sync(self, url: str) -> Optional[str]:
        """ë™ê¸° í˜ì´ì§€ ìš”ì²­ (í´ë°±)"""
        try:
            import requests
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.text
            else:
                print(f"âš ï¸ HTTP {response.status_code}: {url}")
                return None
        except Exception as e:
            print(f"âš ï¸ ë™ê¸° í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}")
            return None
    
    async def search_products_async(self, keyword: str, limit: int = 20) -> List[Dict]:
        """ë¹„ë™ê¸° ìƒí’ˆ ê²€ìƒ‰"""
        start_time = time.time()
        
        # 1. ìºì‹œ í™•ì¸ (ìºì‹œ ë§¤ë‹ˆì €ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if CACHE_AVAILABLE:
            cached_results = cache_manager.get_cached_results(keyword, limit)
            if cached_results:
                return cached_results[:limit]
        
        # 2. ë°ì´í„°ë² ì´ìŠ¤ ê¸°ëŠ¥ ì œê±° (DB í…Œì´ë¸” ì—†ìŒ)
        
        # 3. ì‹¤ì œ í¬ë¡¤ë§ ìˆ˜í–‰
        print(f"ğŸ•·ï¸ ì‹¤ì œ í¬ë¡¤ë§ ì‹œì‘: {keyword}")
        
        try:
            encoded_keyword = quote(keyword)
            search_url = f"https://www.ssg.com/search.ssg?target=all&query={encoded_keyword}&page=1"
            
            # ê²€ìƒ‰ í˜ì´ì§€ ìš”ì²­
            html_content = await self.fetch_page(search_url)
            if not html_content:
                return []
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ìƒí’ˆ ë§í¬ ì¶”ì¶œ
            product_links = soup.select('a[href*="itemView.ssg"][href*="itemId="]')
            
            # ê´‘ê³  ë§í¬ ì œì™¸
            filtered_links = []
            for link in product_links[:limit * 5]:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                if ('advertBidId' not in href and 
                    'ADAD' not in link_text and
                    'advertExtensTeryDivCd' not in href):
                    filtered_links.append(link)
                    if len(filtered_links) >= limit * 2:
                        break
            
            print(f"ğŸ”— ìœ íš¨í•œ ìƒí’ˆ ë§í¬ {len(filtered_links)}ê°œ ë°œê²¬")
            
            # ë³‘ë ¬ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
            tasks = []
            for link in filtered_links[:limit * 2]:
                task = self.extract_product_info_async(link, keyword)
                tasks.append(task)
            
            # ëª¨ë“  ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ì„±ê³µí•œ ê²°ê³¼ë§Œ í•„í„°ë§
            products = []
            for result in results:
                if isinstance(result, dict) and result.get('name'):
                    products.append(result)
                    if len(products) >= limit:
                        break
            
            elapsed_time = time.time() - start_time
            print(f"âš¡ ë¹„ë™ê¸° í¬ë¡¤ë§ ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ, {len(products)}ê°œ ìƒí’ˆ")
            
            # 4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê¸°ëŠ¥ ì œê±° (DB í…Œì´ë¸” ì—†ìŒ)
                
            # 5. ìºì‹œì— ì €ì¥ (ìºì‹œê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if products and CACHE_AVAILABLE:
                cache_manager.cache_results(keyword, products, limit)
            
            return products
            
        except Exception as e:
            print(f"âŒ ë¹„ë™ê¸° í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    async def extract_product_info_async(self, link, keyword: str) -> Optional[Dict]:
        """ë¹„ë™ê¸° ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            href = link.get('href')
            if href.startswith('/'):
                product_url = f"https://www.ssg.com{href}"
            else:
                product_url = href
            
            # ìƒí’ˆëª… ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì‚¬ìš©)
            name = self.extract_product_name_fast(link, keyword)
            
            # ê°€ê²© ì¶”ì¶œ
            price = 0
            current = link.parent
            for _ in range(3):
                if current:
                    price_text = current.get_text()
                    price = self.extract_price_fast(price_text)
                    if price > 0:
                        break
                    current = current.parent
                else:
                    break
            
            # ë¸Œëœë“œ ì¶”ì¶œ
            brand = self.extract_brand_fast(link, name)
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_url = None
            current = link.parent
            for _ in range(2):
                if current:
                    img = current.find('img')
                    if img:
                        image_url = img.get('src') or img.get('data-src')
                        if image_url:
                            if image_url.startswith('//'):
                                image_url = f"https:{image_url}"
                            elif image_url.startswith('/'):
                                image_url = f"https://www.ssg.com{image_url}"
                            break
                    current = current.parent
                else:
                    break
            
            return {
                'name': name.strip(),
                'price': price,
                'url': product_url,
                'image_url': image_url,
                'brand': brand,
                'source': 'SSG'
            }
            
        except Exception as e:
            return None
    
    def extract_product_name_fast(self, link, keyword: str) -> str:
        """ë¹ ë¥¸ ìƒí’ˆëª… ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)"""
        try:
            # ëª¨ë“  í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            all_texts = []
            
            link_text = link.get_text(strip=True)
            if link_text:
                all_texts.append(link_text)
            
            current = link.parent
            for level in range(8):
                if current:
                    text_nodes = current.find_all(text=True)
                    for text_node in text_nodes:
                        text = text_node.strip()
                        if text and len(text) > 5:
                            all_texts.append(text)
                    
                    for tag in ['span', 'div', 'p', 'h1', 'h2', 'h3', 'strong', 'em']:
                        elements = current.find_all(tag)
                        for elem in elements[:10]:
                            elem_text = elem.get_text(strip=True)
                            if elem_text and len(elem_text) > 5:
                                all_texts.append(elem_text)
                    
                    current = current.parent
                else:
                    break
            
            # ìµœì  ìƒí’ˆëª… ì„ íƒ
            best_candidates = []
            
            for text in all_texts:
                if not text or len(text) < 10:
                    continue
                
                cleaned_text = self.clean_text_line(text)
                if not cleaned_text or len(cleaned_text) < 10:
                    continue
                
                score = self.calculate_product_name_score(cleaned_text, keyword)
                if score > 0:
                    best_candidates.append((cleaned_text, score))
            
            if best_candidates:
                best_candidates.sort(key=lambda x: x[1], reverse=True)
                best_name = best_candidates[0][0]
                return self.clean_product_name(best_name[:120])
            
            return f"{keyword} ê´€ë ¨ ìƒí’ˆ"
            
        except Exception:
            return f"{keyword} ê´€ë ¨ ìƒí’ˆ"
    
    def clean_text_line(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ë¼ì¸ ì •ì œ"""
        if not text:
            return ""
        
        text = re.sub(r'íŒë§¤ê°€ê²©.*', '', text)
        text = re.sub(r'(\d{1,3}(?:,\d{3})*ì›).*', '', text)
        text = re.sub(r'(ë¦¬ë·°|ë³„ì |í• ì¸|ë°°ì†¡|ë¬´ë£Œë°°ì†¡|ë‹¹ì¼ë°°ì†¡|íƒë°°).*', '', text)
        text = re.sub(r'(ì¹´ë“œí• ì¸|ì¦‰ì‹œí• ì¸|ì¶”ê°€í• ì¸|ì‚¬ì€í’ˆ|ì¦ì •).*', '', text)
        text = re.sub(r'(100gë‹¹|ê°œë‹¹|mlë‹¹).*', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def calculate_product_name_score(self, text: str, keyword: str) -> int:
        """ìƒí’ˆëª… ì í•©ì„± ì ìˆ˜ ê³„ì‚°"""
        if not text:
            return 0
        
        score = 10
        text_lower = text.lower()
        keyword_lower = keyword.lower()
        
        if keyword_lower in text_lower:
            score += 50
        
        brands = [
            'apple', 'ì‚¼ì„±', 'samsung', 'lg', 'ë‚˜ì´í‚¤', 'nike', 'adidas', 
            'ë†ì‹¬', 'ì˜¤ëšœê¸°', 'ì‚¼ì–‘', 'íŒ”ë„', 'ë¡¯ë°', 'lotte', 'cj'
        ]
        
        for brand in brands:
            if brand in text_lower:
                score += 30
                break
        
        if re.search(r'[A-Z0-9]{3,}', text):
            score += 15
        
        if any(word in text_lower for word in ['ì›', 'ë¦¬ë·°', 'ë³„ì ', 'í• ì¸', 'ë°°ì†¡']):
            score -= 20
        
        if len(text) < 15 or len(text) > 150:
            score -= 10
        
        return max(0, score)
    
    def extract_price_fast(self, text: str) -> int:
        """ë¹ ë¥¸ ê°€ê²© ì¶”ì¶œ"""
        if not text:
            return 0
        
        price_match = re.search(r'(\d{1,3}(?:,\d{3})*)\s*ì›', text)
        if price_match:
            try:
                price = int(price_match.group(1).replace(',', ''))
                if 100 <= price <= 10000000:
                    return price
            except:
                pass
        
        return 0
    
    def extract_brand_fast(self, link, product_name: str) -> str:
        """ë¹ ë¥¸ ë¸Œëœë“œ ì¶”ì¶œ"""
        try:
            if product_name:
                known_brands = [
                    'APPLE', 'ì‚¼ì„±', 'SAMSUNG', 'LG', 'ë‚˜ì´í‚¤', 'NIKE', 
                    'ë†ì‹¬', 'ì˜¤ëšœê¸°', 'ì‚¼ì–‘', 'ì•„ë””ë‹¤ìŠ¤', 'ADIDAS'
                ]
                
                product_upper = product_name.upper()
                for brand in known_brands:
                    if brand.upper() in product_upper:
                        return brand
                
                first_word = product_name.split()[0] if product_name.split() else ''
                if first_word and 2 < len(first_word) < 15:
                    return first_word
            
            return 'ë¸Œëœë“œ ì •ë³´ ì—†ìŒ'
            
        except Exception:
            return 'ë¸Œëœë“œ ì •ë³´ ì—†ìŒ'
    
    def clean_product_name(self, name: str) -> str:
        """ìƒí’ˆëª… ì •ì œ"""
        if not name:
            return name
        
        name = re.sub(r'(ë¦¬ë·°\s*\d+|ë³„ì |í• ì¸|ë°°ì†¡)', '', name)
        name = re.sub(r'(ì •ìƒê°€ê²©|íŒë§¤ê°€ê²©|ìµœê³ íŒë§¤ê°€)', '', name)
        name = re.sub(r'(ADADë€\?\s*íˆ´íŒ\s*ì—´ê¸°)', '', name)
        name = re.sub(r'(ê²€ìƒ‰\s*í•„í„°|ë¸Œëœë“œ\s*ì „ì²´ë³´ê¸°)', '', name)
        name = re.sub(r'(\d+ë§Œì›\s*ì´í•˜|\d+ë§Œì›~\d+ë§Œì›)', '', name)
        name = re.sub(r'(ì¸ê¸°|BEST|ì¶”ì²œ|ì„ ë¬¼)', '', name)
        name = re.sub(r'(ìƒˆë²½ë°°ì†¡|ë‹¹ì¼ë°°ì†¡|ë¬´ë£Œë°°ì†¡)', '', name)
        name = re.sub(r'\s+', ' ', name).strip()
        
        if len(name) > 60:
            name = name[:60] + "..."
        
        return name
    
# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ í•¨ìˆ˜ë“¤ ì œê±°ë¨ (DB í…Œì´ë¸” ì—†ìŒ)

# ë¹„ë™ê¸° ê²€ìƒ‰ í•¨ìˆ˜
async def search_products_async(keyword: str, limit: int = 20) -> List[Dict]:
    """ë¹„ë™ê¸° ìƒí’ˆ ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜"""
    async with AsyncSSGCrawler(max_concurrent=10) as crawler:
        return await crawler.search_products_async(keyword, limit)

# ë™ê¸° ë˜í¼ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±)
def search_ssg_products_enhanced(keyword: str, page: int = 1, limit: int = 20) -> List[Dict]:
    """í–¥ìƒëœ SSG ìƒí’ˆ ê²€ìƒ‰ (ë¹„ë™ê¸° + ìºì‹± + DB)"""
    try:
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            results = loop.run_until_complete(search_products_async(keyword, limit))
            return results
        finally:
            loop.close()
            
    except Exception as e:
        print(f"âŒ í–¥ìƒëœ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        # ê¸°ì¡´ ë™ê¸° ë°©ì‹ìœ¼ë¡œ í´ë°±
        from crawler import search_ssg_products
        return search_ssg_products(keyword, page, limit)